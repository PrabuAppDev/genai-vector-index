{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b77a78-739f-4a28-a6f4-0678b15a713f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: qdrant-client in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (1.12.0)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from qdrant-client) (1.66.2)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from qdrant-client) (1.66.2)\n",
      "Requirement already satisfied: httpx[http2]>=0.20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from qdrant-client) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\programdata\\anaconda3\\lib\\site-packages (from qdrant-client) (1.26.4)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from qdrant-client) (2.10.1)\n",
      "Collecting pydantic>=1.10.8 (from qdrant-client)\n",
      "  Obtaining dependency information for pydantic>=1.10.8 from https://files.pythonhosted.org/packages/df/e4/ba44652d562cbf0bf320e0f3810206149c8a4e99cdbf66da82e97ab53a15/pydantic-2.9.2-py3-none-any.whl.metadata\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from qdrant-client) (2.2.3)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (5.28.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (68.0.0)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (1.3.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\programdata\\anaconda3\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (305.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from pydantic>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from pydantic>=1.10.8->qdrant-client) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic>=1.10.8->qdrant-client) (4.11.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.0.0)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.7\n",
      "    Uninstalling pydantic-1.10.7:\n",
      "      Successfully uninstalled pydantic-1.10.7\n",
      "Successfully installed pydantic-2.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-cloud-auth 0.1.3 requires pydantic<2.0, but you have pydantic 2.9.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-index in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (0.11.17)\n",
      "Requirement already satisfied: qdrant-client in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (1.12.0)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.4 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index) (0.3.4)\n",
      "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.1 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.17 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index) (0.11.17)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.4 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index) (0.2.5)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index) (0.9.48.post3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.10 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index) (0.2.13)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index) (0.2.2)\n",
      "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.3.0,>=0.2.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index) (0.2.2)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.3.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from qdrant-client) (1.66.2)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from qdrant-client) (1.66.2)\n",
      "Requirement already satisfied: httpx[http2]>=0.20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from qdrant-client) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\programdata\\anaconda3\\lib\\site-packages (from qdrant-client) (1.26.4)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from qdrant-client) (2.9.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from qdrant-client) (2.2.3)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (5.28.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (68.0.0)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (1.3.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.51.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (2024.6.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (3.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (1.14.1)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index) (0.1.2)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from llama-index-readers-llama-parse>=0.3.0->llama-index) (0.5.7)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\programdata\\anaconda3\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (305.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from pydantic>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from pydantic>=1.10.8->qdrant-client) (2.23.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.11.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.5)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.17->llama-index) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.17->llama-index) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.12.0,>=0.11.17->llama-index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\prabu\\appdata\\roaming\\python\\python311\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.17->llama-index) (3.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2023.3)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.17->llama-index) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "!pip install qdrant-client\n",
    "!pip install llama-index qdrant-client --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa92f6ad-8722-4970-a5d6-5c883421ab85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1177\n",
      "Sample document: \n",
      "I usually use \"Algorithms for graphics and image processing\" by\n",
      "Theodosios Pavlidis, but other people here got them same idea and now\n",
      "3 of 4 copies in the libraries have been stolen!\n",
      "\n",
      "Another reference is \"Digital Image Processing\" by Gonzalez and\n",
      "Wintz/Wood, which is widely available but a little expensive ($55\n",
      "here- I just checked today)....\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Fetch the 20 Newsgroups dataset (lightweight and suitable for text-based embeddings).\n",
    "# We'll use two categories: 'sci.space' and 'comp.graphics' for variety.\n",
    "newsgroups_data = fetch_20newsgroups(subset='train', categories=['sci.space', 'comp.graphics'], remove=('headers', 'footers', 'quotes'))\n",
    "documents = newsgroups_data.data\n",
    "\n",
    "# Display the number of documents fetched and a sample\n",
    "print(f\"Number of documents: {len(documents)}\")\n",
    "print(f\"Sample document: {documents[0][:500]}...\")  # Displaying first 500 characters of the first document as a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a64cbcf-7d13-4237-b0ce-3ad1c4a69e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1177 embeddings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import tiktoken  # OpenAI's tokenization library\n",
    "\n",
    "# Set up OpenAI API using an environment variable for security (avoid hardcoding API keys).\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Use tiktoken to tokenize the documents and ensure they don't exceed the token limit\n",
    "# The model's maximum token length is 8192, so we'll split the document into smaller chunks if needed.\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "MAX_TOKENS = 8192\n",
    "\n",
    "# Function to split long documents into smaller chunks\n",
    "def split_into_chunks(text, max_tokens=MAX_TOKENS):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    # Split the tokens into chunks of MAX_TOKENS\n",
    "    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    # Decode the token chunks back to text\n",
    "    return [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "# Function to generate embeddings for text using OpenAI's text-embedding-ada-002 model.\n",
    "# It handles splitting long documents and generating embeddings for each chunk.\n",
    "def get_embeddings(text):\n",
    "    # If the document is too long, split it into chunks\n",
    "    chunks = split_into_chunks(text)\n",
    "    \n",
    "    # Generate embeddings for each chunk and return their average\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        response = client.embeddings.create(model=\"text-embedding-ada-002\", input=chunk)\n",
    "        embeddings.append(response.data[0].embedding)\n",
    "    \n",
    "    # Optionally, you can average the embeddings of the chunks to represent the whole document\n",
    "    avg_embedding = [sum(x) / len(x) for x in zip(*embeddings)]\n",
    "    return avg_embedding\n",
    "\n",
    "# Generate embeddings for each document in the dataset.\n",
    "# This step ensures that documents exceeding the token limit are properly handled.\n",
    "embeddings = [get_embeddings(doc) for doc in documents]\n",
    "\n",
    "# Display the number of embeddings generated\n",
    "print(f\"Generated {len(embeddings)} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee158901-d196-440f-b6be-4a7cfdf24226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid embeddings after cleaning: 1172\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to check for NaN values in embeddings\n",
    "def clean_embeddings(embeddings):\n",
    "    # Filter out any embeddings that contain NaN values\n",
    "    cleaned_embeddings = [embedding for embedding in embeddings if not np.isnan(embedding).any()]\n",
    "    return cleaned_embeddings\n",
    "\n",
    "# Clean up the embeddings to remove any NaN values\n",
    "embeddings = clean_embeddings(embeddings)  # Reassign cleaned embeddings back to the embeddings variable\n",
    "\n",
    "# Display the number of valid embeddings after cleaning\n",
    "print(f\"Number of valid embeddings after cleaning: {len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a67c70b1-7457-4604-ba5e-ac27d6e9f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "# Qdrant is the vector database we're using to store high-dimensional embeddings (vectors).\n",
    "# It allows for fast similarity searches, making it ideal for tasks like retrieval-augmented generation (RAG).\n",
    "\n",
    "# Initialize Qdrant as an in-memory instance to avoid external database dependencies.\n",
    "# This is efficient for testing purposes but can be switched to a persistent store if needed.\n",
    "qdrant = QdrantClient(\":memory:\")\n",
    "\n",
    "# Create a collection in Qdrant to store our document embeddings.\n",
    "# We're specifying the size of each vector and using cosine distance to measure similarity between vectors.\n",
    "vector_size = len(embeddings[0])  # Size of the vector depends on the embeddings created by OpenAI.\n",
    "qdrant.create_collection(\n",
    "    collection_name=\"newsgroups\",  # Name of the collection.\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)  # Cosine similarity is common for NLP tasks.\n",
    ")\n",
    "\n",
    "# Upload the embeddings to Qdrant along with the corresponding documents.\n",
    "# The 'payload' stores metadata (in this case, the original document text) associated with each vector.\n",
    "qdrant.upload_collection(\n",
    "    collection_name=\"newsgroups\",\n",
    "    vectors=embeddings,  # The document embeddings generated by OpenAI.\n",
    "    payload=[{\"document\": doc} for doc in documents],  # Metadata (original documents).\n",
    "    ids=None  # Let Qdrant assign unique IDs automatically.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e29ec996-352d-4c83-82d8-5df2c32a1d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# NetworkX is a Python library used to create and manage graph structures.\n",
    "# Here, we'll use it to create an in-memory graph database to represent semantic relationships between documents.\n",
    "\n",
    "# Initialize an empty graph.\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph. Each node represents a document in the dataset.\n",
    "# The node attributes store the document's text for potential retrieval and querying.\n",
    "for i, doc in enumerate(documents):\n",
    "    G.add_node(i, document=doc)\n",
    "\n",
    "# Add edges between nodes based on some predefined criteria.\n",
    "# For simplicity, we'll add an edge between documents that contain the word 'space' (indicating a topic similarity).\n",
    "# In real-world cases, you might use more sophisticated methods (e.g., semantic similarity).\n",
    "for i in range(len(documents)):\n",
    "    for j in range(i+1, len(documents)):\n",
    "        if 'space' in documents[i] and 'space' in documents[j]:\n",
    "            G.add_edge(i, j)  # Create an edge between related documents.\n",
    "\n",
    "# Optional: Visualize the graph to see the connections between documents (helpful for understanding relationships).\n",
    "# nx.draw(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b4cd547-9bcd-43ef-a257-d7f477a1cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pydantic\n",
      "Version: 2.9.2\n",
      "Summary: Data validation using Python type hints\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Samuel Colvin <s@muelcolvin.com>, Eric Jolibois <em.jolibois@gmail.com>, Hasan Ramezani <hasan.r67@gmail.com>, Adrian Garcia Badaracco <1755071+adriangb@users.noreply.github.com>, Terrence Dorsey <terry@pydantic.dev>, David Montague <david@pydantic.dev>, Serge Matveenko <lig@countzero.co>, Marcelo Trylesinski <marcelotryle@gmail.com>, Sydney Runkle <sydneymarierunkle@gmail.com>, David Hewitt <mail@davidhewitt.io>, Alex Hall <alex.mojaki@gmail.com>\n",
      "License: \n",
      "Location: C:\\Users\\prabu\\AppData\\Roaming\\Python\\Python311\\site-packages\n",
      "Requires: annotated-types, pydantic-core, typing-extensions\n",
      "Required-by: anaconda-cloud-auth, llama-cloud, llama-index-core, openai, qdrant-client\n"
     ]
    }
   ],
   "source": [
    "!pip show pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63098d-f114-4749-befc-30f7f2b6da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6082a56f-f08e-4b16-a63e-4fb6c0a85eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydantic==1.10.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a2833-c0a7-4721-b475-c0356ae881f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38018784-b700-4d71-bb87-2d2ff5f95fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from llama_index.core import VectorStoreIndex, SimpleDocument\n",
    "\n",
    "# Step 1: Create Example Documents In-Memory\n",
    "documents = [\n",
    "    SimpleDocument(text=\"Document 1 about space exploration.\"),\n",
    "    SimpleDocument(text=\"Document 2 about advances in AI.\"),\n",
    "    SimpleDocument(text=\"Document 3 about satellites and their role in space technology.\"),\n",
    "    SimpleDocument(text=\"Document 4 about space and science.\")\n",
    "]\n",
    "\n",
    "# Step 2: Create a Vector Store Index from Documents\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Step 3: Set up a Graph Database using NetworkX\n",
    "# We create a graph to represent relationships between the documents.\n",
    "graph_db = nx.Graph()\n",
    "\n",
    "# Add nodes (documents) to the graph\n",
    "for i, doc in enumerate(documents):\n",
    "    graph_db.add_node(i, document=doc.text)\n",
    "\n",
    "# Add edges (relationships between documents)\n",
    "# For example, Documents 1 and 4 are both about space, so we connect them.\n",
    "graph_db.add_edge(0, 3)  # Connects Document 1 (space) and Document 4 (space)\n",
    "graph_db.add_edge(2, 0)  # Connects Document 3 (satellites) and Document 1 (space)\n",
    "\n",
    "# Step 4: Function to Combine Results from Vector and Graph Indices\n",
    "def query_combined_system(query):\n",
    "    # 1. Query the vector index for the most similar documents\n",
    "    vector_response = index.query(query)\n",
    "    \n",
    "    # 2. Find related documents from the graph based on the top vector search result\n",
    "    top_doc_id = 0  # Assume top document is document 0 for this example\n",
    "    related_docs = list(graph_db.neighbors(top_doc_id))  # Get related documents from the graph\n",
    "    \n",
    "    # 3. Aggregate results: vector-based and graph-based related documents\n",
    "    result_docs = [vector_response[0].text] + [documents[doc_id].text for doc_id in related_docs]\n",
    "    return result_docs\n",
    "\n",
    "# Step 5: Test the Combined System with a Query\n",
    "query_result = query_combined_system(\"Tell me about space exploration.\")\n",
    "for result in query_result:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
